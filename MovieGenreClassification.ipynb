{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7vhbLW3qMSn",
        "outputId": "534171d1-35b9-4c79-f113-741aad1e31ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.47.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Downloading streamlit-1.47.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m144.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.47.1 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn streamlit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv('/content/train_data.txt', sep='\\t', names=['id', 'title', 'genre'], skiprows=1)\n",
        "test_data = pd.read_csv('/content/test_data.txt', sep='\\t', names=['id', 'title'], skiprows=1)\n",
        "test_solution = pd.read_csv('/content/test_data_solution.txt', sep='\\t', names=['id', 'genre'], skiprows=1)\n",
        "\n",
        "print(\"Training Data Shape:\", train_data.shape)\n",
        "print(\"Test Data Shape:\", test_data.shape)\n",
        "print(\"Test Solution Shape:\", test_solution.shape)\n",
        "print(\"\\nTraining Data Sample:\\n\", train_data.head())\n",
        "print(\"\\nUnique Genres:\\n\", train_data['genre'].unique())\n",
        "print(\"\\nMissing Values in Train:\\n\", train_data.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiX8k3Dnr0jl",
        "outputId": "cf224058-480f-4fb6-f26a-5186b82b2ce7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Shape: (54213, 3)\n",
            "Test Data Shape: (54199, 2)\n",
            "Test Solution Shape: (54199, 2)\n",
            "\n",
            "Training Data Sample:\n",
            "                                                   id  title  genre\n",
            "0  2 ::: Cupid (1997) ::: thriller ::: A brother ...    NaN    NaN\n",
            "1  3 ::: Young, Wild and Wonderful (1980) ::: adu...    NaN    NaN\n",
            "2  4 ::: The Secret Sin (1915) ::: drama ::: To h...    NaN    NaN\n",
            "3  5 ::: The Unrecovered (2007) ::: drama ::: The...    NaN    NaN\n",
            "4  6 ::: Quality Control (2011) ::: documentary :...    NaN    NaN\n",
            "\n",
            "Unique Genres:\n",
            " [nan]\n",
            "\n",
            "Missing Values in Train:\n",
            " id           0\n",
            "title    54213\n",
            "genre    54213\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "train_data = pd.read_csv('/content/train_data.txt', sep='\\t', names=['id', 'title', 'genre'], skiprows=1)\n",
        "test_data = pd.read_csv('/content/test_data.txt', sep='\\t', names=['id', 'title'], skiprows=1)\n",
        "test_solution = pd.read_csv('/content/test_data_solution.txt', sep='\\t', names=['id', 'genre'], skiprows=1)\n",
        "\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        return text.strip()\n",
        "    return ''\n",
        "train_data['cleaned_text'] = train_data['title'].apply(clean_text)\n",
        "\n",
        "print(\"Sample of cleaned text:\\n\", train_data['cleaned_text'].head(10))\n",
        "print(\"\\nEmpty cleaned text count:\", (train_data['cleaned_text'] == '').sum())\n",
        "print(\"\\nNon-empty cleaned text sample:\\n\", train_data[train_data['cleaned_text'] != ''][['title', 'cleaned_text']].head())\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "try:\n",
        "    X_train = vectorizer.fit_transform(train_data['cleaned_text'])\n",
        "    print(\"TF-IDF Vocabulary size:\", len(vectorizer.get_feature_names_out()))\n",
        "except ValueError as e:\n",
        "    print(\"Error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru9PHxzOsZ6Q",
        "outputId": "d60aae16-2d03-493a-daef-1fef9be25bab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of cleaned text:\n",
            " 0    \n",
            "1    \n",
            "2    \n",
            "3    \n",
            "4    \n",
            "5    \n",
            "6    \n",
            "7    \n",
            "8    \n",
            "9    \n",
            "Name: cleaned_text, dtype: object\n",
            "\n",
            "Empty cleaned text count: 54213\n",
            "\n",
            "Non-empty cleaned text sample:\n",
            " Empty DataFrame\n",
            "Columns: [title, cleaned_text]\n",
            "Index: []\n",
            "Error: empty vocabulary; perhaps the documents only contain stop words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "train_data = pd.read_csv('/content/train_data.txt', sep='\\t', names=['id', 'title', 'genre'], skiprows=1)\n",
        "test_data = pd.read_csv('/content/test_data.txt', sep='\\t', names=['id', 'title'], skiprows=1)\n",
        "test_solution = pd.read_csv('/content/test_data_solution.txt', sep='\\t', names=['id', 'genre'], skiprows=1)\n",
        "\n",
        "def extract_info(row):\n",
        "    parts = row['id'].split(' ::: ')\n",
        "    if len(parts) > 2:\n",
        "        row['title'] = parts[1]\n",
        "        row['genre'] = parts[2]\n",
        "    return row\n",
        "\n",
        "train_data = train_data.apply(extract_info, axis=1)\n",
        "test_data = test_data.apply(extract_info, axis=1)\n",
        "train_data = train_data.drop('id', axis=1).dropna(subset=['title', 'genre'])\n",
        "test_data = test_data.drop('id', axis=1).dropna(subset=['title'])\n",
        "\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        return text.strip()\n",
        "    return ''\n",
        "\n",
        "train_data['cleaned_text'] = train_data['title'].apply(clean_text)\n",
        "test_data['cleaned_text'] = test_data['title'].apply(clean_text)\n",
        "\n",
        "print(\"Empty cleaned text count:\", (train_data['cleaned_text'] == '').sum())\n",
        "print(\"Non-empty cleaned text sample:\\n\", train_data[train_data['cleaned_text'] != ''][['title', 'cleaned_text']].head())\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "try:\n",
        "    X_train = vectorizer.fit_transform(train_data['cleaned_text'])\n",
        "    X_test = vectorizer.transform(test_data['cleaned_text'])\n",
        "    print(\"TF-IDF Vocabulary size:\", len(vectorizer.get_feature_names_out()))\n",
        "except ValueError as e:\n",
        "    print(\"Error:\", e)\n",
        "\n",
        "if 'X_train' in locals() and X_train.shape[1] > 0:\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "    X_train = vectorizer.fit_transform(train_data['cleaned_text'])\n",
        "    X_test = vectorizer.transform(test_data['cleaned_text'])\n",
        "    print(\"TF-IDF Vocabulary size (with stop words):\", len(vectorizer.get_feature_names_out()))\n",
        "else:\n",
        "    print(\"Could not create TF-IDF vectors. Check data and cleaning process.\")\n",
        "\n",
        "y_train = train_data['genre']\n",
        "y_test = test_solution['genre']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyDJj9OetP5i",
        "outputId": "3a1cb22a-8296-45b3-9b2f-a688a1ca9fb6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty cleaned text count: 0\n",
            "Non-empty cleaned text sample:\n",
            "                               title                   cleaned_text\n",
            "0                      Cupid (1997)                     cupid 1997\n",
            "1  Young, Wild and Wonderful (1980)  young wild and wonderful 1980\n",
            "2             The Secret Sin (1915)            the secret sin 1915\n",
            "3            The Unrecovered (2007)           the unrecovered 2007\n",
            "4            Quality Control (2011)           quality control 2011\n",
            "TF-IDF Vocabulary size: 5000\n",
            "TF-IDF Vocabulary size (with stop words): 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    train_data = pd.read_csv('/content/train_data.txt', sep='\\t', names=['id', 'title', 'genre'], skiprows=1)\n",
        "    test_data = pd.read_csv('/content/test_data.txt', sep='\\t', names=['id', 'title'], skiprows=1)\n",
        "    test_solution = pd.read_csv('/content/test_data_solution.txt', sep='\\t', names=['id', 'genre'], skiprows=1)\n",
        "except pd.errors.ParserError:\n",
        "    print(\"Error loading files. Please check the delimiter or file structure.\")\n",
        "    print(\"train_data.txt sample:\")\n",
        "    !head /content/train_data.txt\n",
        "    print(\"\\ntest_data.txt sample:\")\n",
        "    !head /content/test_data.txt\n",
        "    print(\"\\ntest_data_solution.txt sample:\")\n",
        "    !head /content/test_data_solution.txt\n",
        "    raise\n",
        "print(\"NaN values in train_data:\\n\", train_data.isnull().sum())\n",
        "print(\"NaN values in test_data:\\n\", test_data.isnull().sum())\n",
        "print(\"NaN values in test_solution:\\n\", test_solution.isnull().sum())\n",
        "print(\"\\nTrain Data Sample:\\n\", train_data.head())\n",
        "print(\"\\nTest Data Sample:\\n\", test_data.head())\n",
        "print(\"\\nTest Solution Sample:\\n\", test_solution.head())\n",
        "\n",
        "train_data = train_data.dropna(subset=['genre'])\n",
        "test_solution = test_solution.dropna(subset=['genre'])\n",
        "\n",
        "if len(test_data) != len(test_solution):\n",
        "    print(f\"Warning: test_data ({len(test_data)}) and test_solution ({len(test_solution)}) have different lengths.\")\n",
        "    test_data = test_data[test_data['id'].isin(test_solution['id'])]\n",
        "    test_solution = test_solution[test_solution['id'].isin(test_data['id'])]\n",
        "    print(f\"After alignment: test_data ({len(test_data)}), test_solution ({len(test_solution)})\")\n",
        "\n",
        "print(\"\\nNaN values in train_data after cleaning:\\n\", train_data.isnull().sum())\n",
        "print(\"NaN values in test_solution after cleaning:\\n\", test_solution.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0qCpEy8tUCK",
        "outputId": "cb29989b-43b8-4ba5-b949-c4c3fdb2da77"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN values in train_data:\n",
            " id           0\n",
            "title    54213\n",
            "genre    54213\n",
            "dtype: int64\n",
            "NaN values in test_data:\n",
            " id           0\n",
            "title    54199\n",
            "dtype: int64\n",
            "NaN values in test_solution:\n",
            " id           0\n",
            "genre    54199\n",
            "dtype: int64\n",
            "\n",
            "Train Data Sample:\n",
            "                                                   id  title  genre\n",
            "0  2 ::: Cupid (1997) ::: thriller ::: A brother ...    NaN    NaN\n",
            "1  3 ::: Young, Wild and Wonderful (1980) ::: adu...    NaN    NaN\n",
            "2  4 ::: The Secret Sin (1915) ::: drama ::: To h...    NaN    NaN\n",
            "3  5 ::: The Unrecovered (2007) ::: drama ::: The...    NaN    NaN\n",
            "4  6 ::: Quality Control (2011) ::: documentary :...    NaN    NaN\n",
            "\n",
            "Test Data Sample:\n",
            "                                                   id  title\n",
            "0  2 ::: La guerra de papá (1977) ::: Spain, Marc...    NaN\n",
            "1  3 ::: Off the Beaten Track (2010) ::: One year...    NaN\n",
            "2  4 ::: Meu Amigo Hindu (2015) ::: His father ha...    NaN\n",
            "3  5 ::: Er nu zhai (1955) ::: Before he was know...    NaN\n",
            "4  6 ::: Riddle Room (2016) ::: Emily Burns is be...    NaN\n",
            "\n",
            "Test Solution Sample:\n",
            "                                                   id  genre\n",
            "0  2 ::: La guerra de papá (1977) ::: comedy ::: ...    NaN\n",
            "1  3 ::: Off the Beaten Track (2010) ::: document...    NaN\n",
            "2  4 ::: Meu Amigo Hindu (2015) ::: drama ::: His...    NaN\n",
            "3  5 ::: Er nu zhai (1955) ::: drama ::: Before h...    NaN\n",
            "4  6 ::: Riddle Room (2016) ::: horror ::: Emily ...    NaN\n",
            "Warning: test_data (54199) and test_solution (0) have different lengths.\n",
            "After alignment: test_data (0), test_solution (0)\n",
            "\n",
            "NaN values in train_data after cleaning:\n",
            " id       0\n",
            "title    0\n",
            "genre    0\n",
            "dtype: int64\n",
            "NaN values in test_solution after cleaning:\n",
            " id       0\n",
            "genre    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegressionOVR:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.classes = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros((len(self.classes), n_features))\n",
        "        self.bias = np.zeros(len(self.classes))\n",
        "        for i, cls in enumerate(self.classes):\n",
        "            y_binary = np.where(y == cls, 1, 0)\n",
        "            w = np.zeros(n_features)\n",
        "            b = 0\n",
        "            for _ in range(self.n_iterations):\n",
        "                linear_model = np.dot(X, w) + b\n",
        "                y_pred = self.sigmoid(linear_model)\n",
        "                dw = (1/n_samples) * np.dot(X.T, (y_pred - y_binary))\n",
        "                db = (1/n_samples) * np.sum(y_pred - y_binary)\n",
        "                w -= self.learning_rate * dw\n",
        "                b -= self.learning_rate * db\n",
        "            self.weights[i] = w\n",
        "            self.bias[i] = b\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = np.zeros((X.shape[0], len(self.classes)))\n",
        "        for i in range(len(self.classes)):\n",
        "            probs[:, i] = self.sigmoid(np.dot(X, self.weights[i]) + self.bias[i])\n",
        "        probs /= probs.sum(axis=1, keepdims=True)\n",
        "        return probs\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return self.classes[np.argmax(probs, axis=1)]\n",
        "\n",
        "if 'X_train' in locals() and 'X_test' in locals() and X_train.shape[0] > 0 and X_test.shape[0] > 0:\n",
        "    X_train_dense = X_train.toarray()\n",
        "    X_test_dense = X_test.toarray()\n",
        "    model = LogisticRegressionOVR(learning_rate=0.1, n_iterations=1000)\n",
        "    model.fit(X_train_dense, y_train)\n",
        "    y_pred = model.predict(X_test_dense)\n",
        "    from sklearn.metrics import accuracy_score, classification_report\n",
        "    try:\n",
        "        print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "        print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "    except ValueError as e:\n",
        "        print(\"Evaluation error:\", e)\n",
        "        print(\"Sample y_test:\\n\", y_test.head(10))\n",
        "        print(\"Sample y_pred:\\n\", y_pred[:10])\n",
        "else:\n",
        "    print(\"Cannot train model: X_train or X_test is empty or not defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZEajKP54IEW",
        "outputId": "a15b3cd7-aaf3-4288-8b0a-0361d560bb9d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation error: Input y_true contains NaN.\n",
            "Sample y_test:\n",
            " 0   NaN\n",
            "1   NaN\n",
            "2   NaN\n",
            "3   NaN\n",
            "4   NaN\n",
            "5   NaN\n",
            "6   NaN\n",
            "7   NaN\n",
            "8   NaN\n",
            "9   NaN\n",
            "Name: genre, dtype: float64\n",
            "Sample y_pred:\n",
            " ['drama' 'drama' 'drama' 'drama' 'drama' 'drama' 'drama' 'drama' 'drama'\n",
            " 'drama']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py:399: RuntimeWarning: invalid value encountered in cast\n",
            "  return x.astype(dtype, copy=copy, casting=casting)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from google.colab import files\n",
        "joblib.dump(model, 'movie_genre_model.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
        "\n",
        "streamlit_code = \"\"\"\n",
        "import streamlit as st\n",
        "import joblib\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "model = joblib.load('movie_genre_model.pkl')\n",
        "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\\\w\\\\s]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "st.title(\"Movie Genre Classifier\")\n",
        "st.write(\"Enter a movie description or title to predict its genre.\")\n",
        "\n",
        "user_input = st.text_area(\"Movie Description/Title\", \"\")\n",
        "if st.button(\"Predict Genre\"):\n",
        "    if user_input:\n",
        "        cleaned_input = clean_text(user_input)\n",
        "        X_input = vectorizer.transform([cleaned_input]).toarray()\n",
        "        prediction = model.predict(X_input)[0]\n",
        "        st.success(f\"Predicted Genre: {prediction}\")\n",
        "    else:\n",
        "        st.error(\"Please enter a movie description or title.\")\n",
        "\"\"\"\n",
        "\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "files.download('movie_genre_model.pkl')\n",
        "files.download('tfidf_vectorizer.pkl')\n",
        "files.download('app.py')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZAMaCO4qEmaU",
        "outputId": "4d89f7a5-2cc9-49bd-8cd3-5090aedadc43"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_11afb8c5-7d5d-4f63-8253-f2585f69ff54\", \"movie_genre_model.pkl\", 1081122)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6e823109-07d6-4116-b78f-d55099628fe7\", \"tfidf_vectorizer.pkl\", 569)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_39a008b3-ace9-4d00-b242-080861f63abc\", \"app.py\", 772)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(model, 'movie_genre_model.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
        "\n",
        "streamlit_code = \"\"\"\n",
        "import streamlit as st\n",
        "import joblib\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Define LogisticRegressionOVR class\n",
        "class LogisticRegressionOVR:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.classes = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros((len(self.classes), n_features))\n",
        "        self.bias = np.zeros(len(self.classes))\n",
        "        for i, cls in enumerate(self.classes):\n",
        "            y_binary = np.where(y == cls, 1, 0)\n",
        "            w = np.zeros(n_features)\n",
        "            b = 0\n",
        "            for _ in range(self.n_iterations):\n",
        "                linear_model = np.dot(X, w) + b\n",
        "                y_pred = self.sigmoid(linear_model)\n",
        "                dw = (1/n_samples) * np.dot(X.T, (y_pred - y_binary))\n",
        "                db = (1/n_samples) * np.sum(y_pred - y_binary)\n",
        "                w -= self.learning_rate * dw\n",
        "                b -= self.learning_rate * db\n",
        "            self.weights[i] = w\n",
        "            self.bias[i] = b\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = np.zeros((X.shape[0], len(self.classes)))\n",
        "        for i in range(len(self.classes)):\n",
        "            probs[:, i] = self.sigmoid(np.dot(X, self.weights[i]) + self.bias[i])\n",
        "        probs /= probs.sum(axis=1, keepdims=True)\n",
        "        return probs\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return self.classes[np.argmax(probs, axis=1)]\n",
        "\n",
        "# Load model and vectorizer\n",
        "model = joblib.load('movie_genre_model.pkl')\n",
        "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\\\w\\\\s]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "st.title(\"Movie Genre Classifier\")\n",
        "st.write(\"Enter a movie description to predict its genre.\")\n",
        "\n",
        "user_input = st.text_area(\"Movie Description\", \"\")\n",
        "if st.button(\"Predict Genre\"):\n",
        "    if user_input:\n",
        "        cleaned_input = clean_text(user_input)\n",
        "        X_input = vectorizer.transform([cleaned_input]).toarray()\n",
        "        prediction = model.predict(X_input)[0]\n",
        "        st.success(f\"Predicted Genre: {prediction}\")\n",
        "    else:\n",
        "        st.error(\"Please enter a movie description.\")\n",
        "\"\"\"\n",
        "\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write(streamlit_code)"
      ],
      "metadata": {
        "id": "jkfJlCqnFmEE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35g02_P9FpDi",
        "outputId": "1ddb5b98-c3ad-4b08-d98f-cce80687050f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "!ngrok authtoken 2tTHGHJLW3emePMG0ArjsFQxl2Q_3fXqm2GiGCT7FDg2NPkUq\n",
        "!streamlit run app.py &>/dev/null &\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Streamlit app running at:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "collapsed": true,
        "id": "BfnvATDAF0aW",
        "outputId": "9f7e9d36-8b37-4496-f384-21ae5a223669"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-08-05T16:54:11+0000 lvl=warn msg=\"failed to start tunnel\" pg=/api/tunnels id=80b4765cfb825be5 err=\"failed to start tunnel: Your account may not run more than 3 tunnels over a single ngrok agent session.\\nThe tunnels already running on this session are:\\ntn_30sMR6mhpn7Vuwwc8BxcqwOL6is, tn_30sMigTueoXtip9DhXteRiQXNld, tn_30sOEYlTLN7SgPP3u6OjFhDmB0s\\n\\r\\n\\r\\nERR_NGROK_324\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokHTTPError",
          "evalue": "ngrok client exception, API returned 502: {\"error_code\":103,\"status_code\":502,\"msg\":\"failed to start tunnel\",\"details\":{\"err\":\"failed to start tunnel: Your account may not run more than 3 tunnels over a single ngrok agent session.\\nThe tunnels already running on this session are:\\ntn_30sMR6mhpn7Vuwwc8BxcqwOL6is, tn_30sMigTueoXtip9DhXteRiQXNld, tn_30sOEYlTLN7SgPP3u6OjFhDmB0s\\n\\r\\n\\r\\nERR_NGROK_324\\r\\n\"}}\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(url, method, data, params, timeout, auth)\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 502: Bad Gateway",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPyngrokNgrokHTTPError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3732954639.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Create public URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8501\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Streamlit app running at:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     tunnel = NgrokTunnel(api_request(f\"{api_url}/api/tunnels\", method=\"POST\", data=options,\n\u001b[0m\u001b[1;32m    390\u001b[0m                                      timeout=pyngrok_config.request_timeout),\n\u001b[1;32m    391\u001b[0m                          pyngrok_config, api_url)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(url, method, data, params, timeout, auth)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Response {status_code}: {response_data.strip()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         raise PyngrokNgrokHTTPError(f\"ngrok client exception, API returned {status_code}: {response_data}\",\n\u001b[0m\u001b[1;32m    649\u001b[0m                                     \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m                                     status_code, e.reason, e.headers, response_data)\n",
            "\u001b[0;31mPyngrokNgrokHTTPError\u001b[0m: ngrok client exception, API returned 502: {\"error_code\":103,\"status_code\":502,\"msg\":\"failed to start tunnel\",\"details\":{\"err\":\"failed to start tunnel: Your account may not run more than 3 tunnels over a single ngrok agent session.\\nThe tunnels already running on this session are:\\ntn_30sMR6mhpn7Vuwwc8BxcqwOL6is, tn_30sMigTueoXtip9DhXteRiQXNld, tn_30sOEYlTLN7SgPP3u6OjFhDmB0s\\n\\r\\n\\r\\nERR_NGROK_324\\r\\n\"}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "try:\n",
        "    train_data = pd.read_csv('/content/train_data.txt', sep=' ::: ', names=['id', 'title', 'genre', 'description'], engine='python')\n",
        "    test_data = pd.read_csv('/content/test_data.txt', sep=' ::: ', names=['id', 'title', 'description'], engine='python')\n",
        "    test_solution = pd.read_csv('/content/test_data_solution.txt', sep=' ::: ', names=['id', 'title', 'genre', 'description'], engine='python')\n",
        "except pd.errors.ParserError:\n",
        "    print(\"Parsing error. Check file structure.\")\n",
        "    raise\n",
        "\n",
        "print(\"Train Data Columns:\", train_data.columns)\n",
        "print(\"Train Data Sample:\\n\", train_data.head())\n",
        "print(\"\\nTest Data Columns:\", test_data.columns)\n",
        "print(\"Test Data Sample:\\n\", test_data.head())\n",
        "print(\"\\nTest Solution Columns:\", test_solution.columns)\n",
        "print(\"Test Solution Sample:\\n\", test_solution.head())\n",
        "\n",
        "print(\"\\nNaN values in train_data:\\n\", train_data.isnull().sum())\n",
        "print(\"NaN values in test_data:\\n\", test_data.isnull().sum())\n",
        "print(\"NaN values in test_solution:\\n\", test_solution.isnull().sum())\n",
        "\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        return text.strip()\n",
        "    return ''\n",
        "\n",
        "train_data['cleaned_text'] = train_data['description'].apply(clean_text)\n",
        "test_data['cleaned_text'] = test_data['description'].apply(clean_text)\n",
        "\n",
        "print(\"Empty cleaned text in train_data:\", (train_data['cleaned_text'] == '').sum())\n",
        "print(\"Empty cleaned text in test_data:\", (test_data['cleaned_text'] == '').sum())\n",
        "print(\"Sample cleaned text (train_data):\\n\", train_data[['description', 'cleaned_text']].head(10))\n",
        "train_data = train_data[train_data['cleaned_text'] != ''].dropna(subset=['cleaned_text', 'genre'])\n",
        "test_data = test_data[test_data['cleaned_text'] != ''].dropna(subset=['cleaned_text'])\n",
        "test_solution = test_solution.dropna(subset=['genre'])\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "try:\n",
        "    X_train = vectorizer.fit_transform(train_data['cleaned_text'])\n",
        "    X_test = vectorizer.transform(test_data['cleaned_text'])\n",
        "    print(\"TF-IDF Vocabulary size:\", len(vectorizer.get_feature_names_out()))\n",
        "except ValueError as e:\n",
        "    print(\"Vectorization error:\", e)\n",
        "    print(\"Sample cleaned text:\\n\", train_data['cleaned_text'].head(10))\n",
        "    raise\n",
        "\n",
        "y_train = train_data['genre']\n",
        "y_test = test_solution['genre']\n",
        "\n",
        "print(\"NaN in y_train:\", y_train.isnull().sum())\n",
        "print(\"NaN in y_test:\", y_test.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95VbR5saHphJ",
        "outputId": "a503ab98-d693-4372-fa0f-fe393f1e48d2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Columns: Index(['id', 'title', 'genre', 'description'], dtype='object')\n",
            "Train Data Sample:\n",
            "    id                             title     genre  \\\n",
            "0   1      Oscar et la dame rose (2009)     drama   \n",
            "1   2                      Cupid (1997)  thriller   \n",
            "2   3  Young, Wild and Wonderful (1980)     adult   \n",
            "3   4             The Secret Sin (1915)     drama   \n",
            "4   5            The Unrecovered (2007)     drama   \n",
            "\n",
            "                                         description  \n",
            "0  Listening in to a conversation between his doc...  \n",
            "1  A brother and sister with a past incestuous re...  \n",
            "2  As the bus empties the students for their fiel...  \n",
            "3  To help their unemployed father make ends meet...  \n",
            "4  The film's title refers not only to the un-rec...  \n",
            "\n",
            "Test Data Columns: Index(['id', 'title', 'description'], dtype='object')\n",
            "Test Data Sample:\n",
            "    id                        title  \\\n",
            "0   1         Edgar's Lunch (1998)   \n",
            "1   2     La guerra de papá (1977)   \n",
            "2   3  Off the Beaten Track (2010)   \n",
            "3   4       Meu Amigo Hindu (2015)   \n",
            "4   5            Er nu zhai (1955)   \n",
            "\n",
            "                                         description  \n",
            "0  L.R. Brane loves his life - his car, his apart...  \n",
            "1  Spain, March 1964: Quico is a very naughty chi...  \n",
            "2  One year in the life of Albin and his family o...  \n",
            "3  His father has died, he hasn't spoken with his...  \n",
            "4  Before he was known internationally as a marti...  \n",
            "\n",
            "Test Solution Columns: Index(['id', 'title', 'genre', 'description'], dtype='object')\n",
            "Test Solution Sample:\n",
            "    id                        title        genre  \\\n",
            "0   1         Edgar's Lunch (1998)     thriller   \n",
            "1   2     La guerra de papá (1977)       comedy   \n",
            "2   3  Off the Beaten Track (2010)  documentary   \n",
            "3   4       Meu Amigo Hindu (2015)        drama   \n",
            "4   5            Er nu zhai (1955)        drama   \n",
            "\n",
            "                                         description  \n",
            "0  L.R. Brane loves his life - his car, his apart...  \n",
            "1  Spain, March 1964: Quico is a very naughty chi...  \n",
            "2  One year in the life of Albin and his family o...  \n",
            "3  His father has died, he hasn't spoken with his...  \n",
            "4  Before he was known internationally as a marti...  \n",
            "\n",
            "NaN values in train_data:\n",
            " id             0\n",
            "title          0\n",
            "genre          0\n",
            "description    0\n",
            "dtype: int64\n",
            "NaN values in test_data:\n",
            " id             0\n",
            "title          0\n",
            "description    0\n",
            "dtype: int64\n",
            "NaN values in test_solution:\n",
            " id             0\n",
            "title          0\n",
            "genre          0\n",
            "description    0\n",
            "dtype: int64\n",
            "Empty cleaned text in train_data: 0\n",
            "Empty cleaned text in test_data: 0\n",
            "Sample cleaned text (train_data):\n",
            "                                          description  \\\n",
            "0  Listening in to a conversation between his doc...   \n",
            "1  A brother and sister with a past incestuous re...   \n",
            "2  As the bus empties the students for their fiel...   \n",
            "3  To help their unemployed father make ends meet...   \n",
            "4  The film's title refers not only to the un-rec...   \n",
            "5  Quality Control consists of a series of 16mm s...   \n",
            "6  In tough economic times Max and Joey have all ...   \n",
            "7  Ron Petrie (Keanu Reeves) is a troubled teen w...   \n",
            "8  A sudden calamitous event, causing great loss ...   \n",
            "9  Four high school students embark on a terrifyi...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  listening in to a conversation between his doc...  \n",
            "1  a brother and sister with a past incestuous re...  \n",
            "2  as the bus empties the students for their fiel...  \n",
            "3  to help their unemployed father make ends meet...  \n",
            "4  the films title refers not only to the unrecov...  \n",
            "5  quality control consists of a series of 16mm s...  \n",
            "6  in tough economic times max and joey have all ...  \n",
            "7  ron petrie keanu reeves is a troubled teen who...  \n",
            "8  a sudden calamitous event causing great loss o...  \n",
            "9  four high school students embark on a terrifyi...  \n",
            "TF-IDF Vocabulary size: 5000\n",
            "NaN in y_train: 0\n",
            "NaN in y_test: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8dUnraQMVeV",
        "outputId": "5d48a772-2907-4572-a0b9-c536c17bedd2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.15).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move files to the codsoft_1 directory\n",
        "!cp /content/movie_genre_model.pkl /content/codsoft_1/\n",
        "!cp /content/tfidf_vectorizer.pkl /content/codsoft_1/\n",
        "!cp /content/app.py /content/codsoft_1/\n",
        "\n",
        "# Navigate to the repository folder\n",
        "%cd /content/codsoft_1\n",
        "\n",
        "# Add files to Git\n",
        "!git add movie_genre_model.pkl tfidf_vectorizer.pkl app.py requirements.txt\n",
        "\n",
        "# Commit changes\n",
        "!git commit -m \"Add movie genre classifier model, vectorizer, Streamlit app, and requirements\"\n",
        "\n",
        "# Set up Git credentials (replace with your details)\n",
        "!git config --global user.name \"haresh-sai06\"\n",
        "!git config --global user.email \"shareshsainaath@gmail.com\"\n",
        "\n",
        "# Push to GitHub\n",
        "!git push https://<your-username>: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@github.com/haresh-sai06/codsoft_1.git main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYVh6VOaNPZl",
        "outputId": "477f79b3-1778-4819-babc-4a0bd36b2c24"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/codsoft_1\n",
            "On branch main\n",
            "Your branch is based on 'origin/main', but the upstream is gone.\n",
            "  (use \"git branch --unset-upstream\" to fixup)\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31mcodsoft_1/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n",
            "/bin/bash: line 1: your-username: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create requirements.txt\n",
        "requirements = \"\"\"\n",
        "streamlit\n",
        "joblib\n",
        "scikit-learn\n",
        "numpy\n",
        "\"\"\"\n",
        "\n",
        "with open('requirements.txt', 'w') as f:\n",
        "    f.write(requirements)"
      ],
      "metadata": {
        "id": "q1JWSUFKPxLU"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}